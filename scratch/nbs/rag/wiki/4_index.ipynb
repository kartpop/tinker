{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.0 at grpc_health/v1/health.proto. Please avoid checked-in Protobuf gencode that can be obsolete.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'wiki' to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import json\n",
    "import redis\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from haystack import Document\n",
    "from typing import Tuple\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from wiki.lib.index.graph.page_graph_creator import Neo4jPageGraphCreator\n",
    "from wiki.lib.index.graph.category_graph_creator import Neo4jCategoryGraphCreator\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack_integrations.document_stores.weaviate.document_store import WeaviateDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables (kind of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "w_store = WeaviateDocumentStore(url=\"http://localhost:8088\")\n",
    "w_writer = DocumentWriter(document_store=w_store, policy=DuplicatePolicy.SKIP)\n",
    "e_store = ElasticsearchDocumentStore(hosts= \"http://localhost:9200\")\n",
    "e_writer = DocumentWriter(document_store=e_store, policy=DuplicatePolicy.SKIP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_pathname_map(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the title_pathname map from a file. Returns an empty dictionary if the file does not exist.\n",
    "    \"\"\"\n",
    "    title_pathname_filepath = os.path.join(filepath, \".metadata/download/title_pathname.json\")\n",
    "    if not os.path.exists(title_pathname_filepath):\n",
    "        raise FileNotFoundError(f\"The file '{title_pathname_filepath}' does not exist.\")\n",
    "    \n",
    "    with open(title_pathname_filepath, \"r\") as file:\n",
    "        title_pathname = json.load(file)\n",
    "    \n",
    "    return title_pathname\n",
    "\n",
    "def get_documents_and_page_hierarchy(filepath: str, page_title: str, page_filename: str) -> Tuple[List[Document], dict]:\n",
    "    \"\"\"\n",
    "    Extracts the documents and hierarchy of a page from the stored chunks in the .metadata/chunk/{page_filename}.json file.\n",
    "    \"\"\"\n",
    "    page_filename_wo_ext = os.path.splitext(page_filename)[0]\n",
    "    chunk_filepath = os.path.join(filepath, \".metadata/chunk\", f\"{page_filename_wo_ext}.json\")\n",
    "    if not os.path.exists(chunk_filepath):\n",
    "        raise FileNotFoundError(f\"The file '{chunk_filepath}' does not exist.\")\n",
    "    \n",
    "    with open(chunk_filepath, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    if not \"splitter\" in data:\n",
    "        raise KeyError(f\"The 'splitter' key is missing in the chunk file {chunk_filepath}.\")\n",
    "    if not \"documents\" in data[\"splitter\"]:\n",
    "        raise KeyError(f\"The 'documents' key is missing in the 'splitter' key in the chunk file {chunk_filepath}.\")\n",
    "    if not \"hierarchy\" in data[\"splitter\"]:\n",
    "        raise KeyError(f\"The 'hierarchy' key is missing in the 'splitter' key in the chunk file {chunk_filepath}.\")\n",
    "    \n",
    "    documents = [Document.from_dict(doc) for doc in data[\"splitter\"][\"documents\"]]  # convert dict to Haystack Document object\n",
    "    hierarchy = data[\"splitter\"][\"hierarchy\"][page_title]\n",
    "\n",
    "    return documents, hierarchy\n",
    "\n",
    "\n",
    "def store_documents_elasticsearch(documents: List[Document]) -> None:\n",
    "    \"\"\"\n",
    "    Store documents in ElasticsearchDocumentStore.\n",
    "    \"\"\"\n",
    "    e_writer.run(documents=documents)\n",
    "    \n",
    "def get_embedded_documents(documents: List[Document], filepath: str, page_filename: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Get embedded documents from ./metadta/index/embeddings. Create embeddings using the OpenAIDocumentEmbedder and store\n",
    "    the embedded documents in the same directory if they do not already exist.\n",
    "    \"\"\"\n",
    "    page_filename_wo_ext = os.path.splitext(page_filename)[0]\n",
    "    embeddings_filepath = os.path.join(filepath, \".metadata/index/embeddings\", f\"{page_filename_wo_ext}.json\")\n",
    "    \n",
    "    if os.path.exists(embeddings_filepath):\n",
    "        with open(embeddings_filepath, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        return [Document.from_dict(doc) for doc in data[\"documents\"]]\n",
    "    \n",
    "    metadata_emdeddings_path = os.path.join(filepath, \".metadata/index/embeddings\")\n",
    "    if not os.path.exists(metadata_emdeddings_path):\n",
    "        os.makedirs(metadata_emdeddings_path)\n",
    "    \n",
    "    # Create embeddings and store embedded documents\n",
    "    embedded_documents = embedder.run(documents=documents) \n",
    "    if 'documents' not in embedded_documents:\n",
    "        raise KeyError(\"The 'documents' key is missing in the embedded_documents returned from embedder.\")\n",
    "    embedded_docs_file_to_save = {\n",
    "        \"documents\": [doc.to_dict() for doc in embedded_documents[\"documents\"]],    # convert Haystack Document object to dict\n",
    "        \"meta\": embedded_documents[\"meta\"] if 'meta' in embedded_documents else {}\n",
    "    }\n",
    "    with open(embeddings_filepath, \"w\") as file:\n",
    "        json.dump(embedded_docs_file_to_save, file)\n",
    "        \n",
    "    return embedded_documents[\"documents\"]\n",
    "\n",
    "\n",
    "\n",
    "def store_documents_weaviate(documents: List[Document]) -> None:\n",
    "    \"\"\"\n",
    "    store documents in WeaviateDocumentStore.\n",
    "    \"\"\"\n",
    "    w_writer.run(documents=documents)\n",
    "    \n",
    "\n",
    "def index_wiki_pages(category: str, filepath: str, category_pages_indexed: Dict[str, int], page_graph_creator: Neo4jPageGraphCreator, depth: int) -> int:\n",
    "    \"\"\"\n",
    "    Indexes already chunked wiki data for all pages in a category and its subcategories. Chunked data is available in \n",
    "    the .metadata/chunk directory. The intermediate embeddings are stored in the .metadata/index/embeddings directory.\n",
    "    \n",
    "    List of Haystack Document objects is created from stored chunks and stored into three databases:\n",
    "    - ElasticsearchDocumentStore: for full-text search (list of Document objects without embeddings is stored)\n",
    "    - WeaviateDocumentStore: for vector search (list of Document objects enriched with embeddings is stored)\n",
    "    - Neo4j: for graph search (list of Document objects are stored as Chunk type nodes and Section, Page, Category type nodes\n",
    "    are created to represent the structure of the data)\n",
    "    \"\"\"\n",
    "    if depth > 100:\n",
    "        return 0\n",
    "    \n",
    "    title_pathname = get_title_pathname_map(filepath)\n",
    "    \n",
    "    pages_filename_set = {file.name for file in Path(filepath).glob(\"*.html\")}\n",
    "    categories_dirname_set = {dir.name for dir in Path(filepath).iterdir() if dir.is_dir() and dir.name != \".metadata\"}\n",
    "    \n",
    "    num_total_pages_indexed = 0\n",
    "    \n",
    "    pages = title_pathname[\"pages\"]\n",
    "    for page_title, page_filename in pages.items():\n",
    "        if r.sismember(\"indexed_pages\", page_title):\n",
    "            continue\n",
    "        if page_filename not in pages_filename_set:\n",
    "            continue\n",
    "        documents, hierarchy = get_documents_and_page_hierarchy(filepath, page_title, page_filename)\n",
    "        store_documents_elasticsearch(documents)\n",
    "        embedded_documents = get_embedded_documents(documents, filepath, page_filename)\n",
    "        store_documents_weaviate(embedded_documents)\n",
    "        page_graph_creator.create_graph(hierarchy)\n",
    "        r.sadd(\"indexed_pages\", page_title)\n",
    "        num_total_pages_indexed += 1\n",
    "    \n",
    "    if num_total_pages_indexed > 0:\n",
    "        category_pages_indexed[category] = num_total_pages_indexed\n",
    "    \n",
    "    subcategories = title_pathname[\"categories\"]\n",
    "    for subcategory_title, subcategory_path in subcategories.items():\n",
    "        if r.sismember(\"indexed_categories\", subcategory_title):\n",
    "            continue\n",
    "        if subcategory_path not in categories_dirname_set:\n",
    "            continue\n",
    "        subcategory_path = os.path.join(filepath, subcategory_path)\n",
    "        num_total_pages_indexed += index_wiki_pages(subcategory_title, subcategory_path, category_pages_indexed, page_graph_creator, depth+1)\n",
    "        r.sadd(\"indexed_categories\", subcategory_title)\n",
    "    \n",
    "    return num_total_pages_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_graph(category: str, filepath: str, category_graph_creator: Neo4jCategoryGraphCreator) -> int:\n",
    "    \"\"\"\n",
    "    Creates a graph representation of the category and connections to its subcategories and pages. The graph is created \n",
    "    on top of the individual page hierarchy graphs already existing in Neo4j. \n",
    "    \"\"\"\n",
    "    title_pathname = get_title_pathname_map(filepath)\n",
    "    \n",
    "    pages = title_pathname[\"pages\"]\n",
    "    for page_title in pages:\n",
    "        if not r.sismember(\"indexed_pages\", page_title):\n",
    "            continue\n",
    "        category_graph_creator.create_category_to_page_relationship(category, page_title)\n",
    "    \n",
    "    subcategories = title_pathname[\"categories\"]\n",
    "    for subcategory_title, subcategory_path in subcategories.items():\n",
    "        if not r.sismember(\"indexed_categories\", subcategory_title):\n",
    "            continue\n",
    "        category_graph_creator.create_category_to_subcategory_relationship(category, subcategory_title)\n",
    "        subcategory_path = os.path.join(filepath, subcategory_path)\n",
    "        create_category_graph(subcategory_title, subcategory_path, category_graph_creator)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_wiki_data(category: str, filepath: str, page_graph_creator: Neo4jPageGraphCreator, category_graph_creator: Neo4jCategoryGraphCreator, depth: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Indexes the wiki data for a category and its subcategories. The data is indexed in ElasticsearchDocumentStore, \n",
    "    WeaviateDocumentStore, and Neo4j. The graph representation of the category and its subcategories is created in Neo4j.\n",
    "    \"\"\"\n",
    "    \n",
    "    category_pages_indexed = {}\n",
    "    num_total_pages_indexed = index_wiki_pages(category, filepath, category_pages_indexed, page_graph_creator, depth)\n",
    "    create_category_graph(category, filepath, category_graph_creator)\n",
    "    \n",
    "    print(f\"Indexed {num_total_pages_indexed} total pages in the category {category}.\")\n",
    "    print(f\"Split up of pages indexed per subcategory in {category}: {category_pages_indexed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 0 total pages in the category Dinosaurs.\n",
      "Split up of pages indexed per subcategory in Dinosaurs: {}\n"
     ]
    }
   ],
   "source": [
    "page_graph_creator = Neo4jPageGraphCreator(\"bolt://localhost:7687\", \"neo4j\", \"neo4jpass\")\n",
    "category_graph_creator = Neo4jCategoryGraphCreator(\"bolt://localhost:7687\", \"neo4j\", \"neo4jpass\")\n",
    "\n",
    "index_wiki_data(\"Dinosaurs\", \"data/v2/Dinosaurs\", page_graph_creator, category_graph_creator, 99)\n",
    "\n",
    "page_graph_creator.close()\n",
    "category_graph_creator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2415"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_store.count_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2415"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.count_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.delete(\"indexed_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.delete(\"indexed_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"indexed_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"indexed_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=263aed0a-7c9a-4dfb-851c-03ca26ec3687, content: 'Dinosaurs are a diverse group of reptiles of the clade Dinosauria . They first appeared during the T...', meta: {'file_path': 'data/v2/Dinosaurs/Dinosaur.html', 'source_id': '3283d9d1d64425e10055eed8bc2bfb821c10a22b1c4c33964b651d49788b7918', 'split_id': 0, 'title': 'Dinosaur'}, score: 0.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.filter_documents(filters = {\"field\": \"id\", \"operator\": \"==\", \"value\": \"263aed0a-7c9a-4dfb-851c-03ca26ec3687\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=8be9010c-85a1-40b8-bd5c-95b290b9f71b, content: 'Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed...', meta: {'file_path': 'data/v2/Dinosaurs/Dinosaur.html', 'source_id': '3283d9d1d64425e10055eed8bc2bfb821c10a22b1c4c33964b651d49788b7918', 'split_id': 201, 'title': 'Dinosaur', 'h2': 'Paleobiology', 'h3': 'Size', 'h4': 'Largest and smallest'}, score: 0.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.filter_documents(filters = {\"field\": \"id\", \"operator\": \"==\", \"value\": \"8be9010c-85a1-40b8-bd5c-95b290b9f71b\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
