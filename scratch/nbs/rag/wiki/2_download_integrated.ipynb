{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import requests\n",
    "import json\n",
    "import redis\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_category_members(category: str, filepath: str) -> list:\n",
    "    \"\"\"\n",
    "    Returns the list of pages and sub-categories of a given Wikipedia category.\n",
    "    \"\"\"\n",
    "    metadata_download_path = os.path.join(filepath, \".metadata/download\")\n",
    "\n",
    "    # Create metadata folder if it doesn't exist and get category information\n",
    "    if not os.path.exists(metadata_download_path):\n",
    "        os.makedirs(metadata_download_path)\n",
    "\n",
    "        # Make the initial request to get the category information\n",
    "        url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmtitle=Category:{category}&cmlimit=max\"\n",
    "        headers = {\"User-Agent\": \"Tinker/0.1 (kartikeyapophali@gmail.com)\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        write_response_data = response.json()\n",
    "\n",
    "        response_filepath = os.path.join(metadata_download_path, \"response.json\")\n",
    "        with open(response_filepath, \"w\") as file:\n",
    "            json.dump(write_response_data, file)\n",
    "\n",
    "    # Read the response data from the file\n",
    "    response_file = os.path.join(metadata_download_path, \"response.json\")\n",
    "    with open(response_file, \"r\") as file:\n",
    "        response_data = json.load(file)\n",
    "\n",
    "    if \"query\" not in response_data or \"categorymembers\" not in response_data[\"query\"]:\n",
    "        return []\n",
    "\n",
    "    return response_data[\"query\"][\"categorymembers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_pathname_map(category: str, filepath: str, inverse_filter: List[str] = []) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a map of page_title to file name and category title to directory name derived from Wikipedia response query. \n",
    "    Pages/categories in the inverse_filter list are not included. \n",
    "    \"\"\"\n",
    "    metadata_download_path = os.path.join(filepath, \".metadata/download\")\n",
    "\n",
    "    title_pathname_filepath = os.path.join(\n",
    "        metadata_download_path, \"title_pathname.json\"\n",
    "    )\n",
    "    if not os.path.exists(title_pathname_filepath):\n",
    "        title_pathname = {\"pages\": {}, \"categories\": {}}\n",
    "\n",
    "        category_members = wiki_category_members(category, filepath)\n",
    "\n",
    "        for member in category_members:\n",
    "            # Skip members to filter out based on keywords or phrases in inverse_filter\n",
    "            # Example: inverse_filter = [\"birds\", \"list of\"] will filter out all pages/categories with \"birds\" or \"list of\" in their title\n",
    "            if any(\n",
    "                keyword.lower() in member[\"title\"].lower() for keyword in inverse_filter\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            if member[\"ns\"] == 0:\n",
    "                page_title = member[\"title\"]\n",
    "                underscored_page_title = page_title.replace(\" \", \"_\")\n",
    "                page_filename = f\"{underscored_page_title}.html\"\n",
    "                title_pathname[\"pages\"][page_title] = page_filename\n",
    "            elif member[\"ns\"] == 14:\n",
    "                category_title = member[\"title\"].replace(\"Category:\", \"\")\n",
    "                underscored_category_title = category_title.replace(\" \", \"_\")\n",
    "                category_dirname = f\"{underscored_category_title}\"\n",
    "                title_pathname[\"categories\"][category_title] = category_dirname\n",
    "\n",
    "        with open(title_pathname_filepath, \"w\") as file:\n",
    "            json.dump(title_pathname, file)\n",
    "        return title_pathname\n",
    "\n",
    "    with open(title_pathname_filepath, \"r\") as file:\n",
    "        title_pathname = json.load(file)\n",
    "\n",
    "    return title_pathname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(page_title: str, page_filename: str, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Fetches the page content from Wikipedia, saves it in an HTML file in the specified directory, and returns the file name.\n",
    "    \"\"\"\n",
    "    wiki_html = wikipediaapi.Wikipedia(\n",
    "        user_agent='Tinker/0.1 (kartikeyapophali@gmail.com)',\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.HTML\n",
    "    )\n",
    "\n",
    "    p_html = wiki_html.page(page_title)\n",
    "    file_path = os.path.join(filepath, page_filename)\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(p_html.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wiki_data(category: str, filepath: str, inverse_filter: List[str], category_pages_downloaded: dict, depth: int) -> int:\n",
    "    if depth > 100:\n",
    "        return 0\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    \n",
    "    num_total_pages_downloaded = 0    \n",
    "    title_pathname = title_pathname_map(category, filepath, inverse_filter)\n",
    "    \n",
    "    pages = title_pathname[\"pages\"]\n",
    "    for page_title, page_filename in pages.items():\n",
    "        if r.sismember(\"downloaded_pages\", page_title):\n",
    "            continue\n",
    "        download_page(page_title, page_filename, filepath)\n",
    "        num_total_pages_downloaded += 1\n",
    "        r.sadd(\"downloaded_pages\", page_title)\n",
    "        \n",
    "    if num_total_pages_downloaded > 0:\n",
    "        category_pages_downloaded[category] = num_total_pages_downloaded\n",
    "    \n",
    "    subcategories = title_pathname[\"categories\"]\n",
    "    for subcategory_title, subcategory_path in subcategories.items():\n",
    "        if r.sismember(\"downloaded_categories\", subcategory_title):\n",
    "            continue\n",
    "        subcategory_path = os.path.join(filepath, subcategory_path)\n",
    "        subcategory_total_pages_downloaded = fetch_wiki_data(subcategory_title, subcategory_path, inverse_filter, category_pages_downloaded, depth + 1)\n",
    "        num_total_pages_downloaded += subcategory_total_pages_downloaded\n",
    "        r.sadd(\"downloaded_categories\", subcategory_title)\n",
    "    \n",
    "    return num_total_pages_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_pages_downloaded = {}\n",
    "pages_downloaded = fetch_wiki_data(\"Dinosaurs\", \"data/v2/Dinosaurs\", [\"bird\", \"list of\", \"lists of\"], category_pages_downloaded, 99)\n",
    "\n",
    "pages_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dinosaurs': 11,\n",
       " 'Dinosaur-related lists': 3,\n",
       " 'Dinosaur paleontology': 3,\n",
       " 'Dinosaurs in popular culture': 14,\n",
       " 'Dinosaur taxonomy': 1,\n",
       " 'Ornithischians': 3,\n",
       " 'Saurischians': 11,\n",
       " 'Dinosaur stubs': 63}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_pages_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"downloaded_pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.scard(\"downloaded_categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: There is no conflict between count of ```category_pages_downloaded``` and count of ```downloaded_categories``` in redis. Code logic does not add category in the dict if page count is 0. In this sample run, depth is restricted to 2, therefore 3rd level categories haven't been processed, but get added in redis' downloaded_categories.***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.delete(\"downloaded_pages\")\n",
    "r.delete(\"downloaded_categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***INCLUDE ERROR HANDLING IN PRODUCTION CODE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
