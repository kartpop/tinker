{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def WikiPageChunks(html_str: str) -> list:\n",
    "    soup = BeautifulSoup(html_str, 'html.parser')  # Parse the HTML content\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    def clean_text(text):\n",
    "        cleaned_text = text.replace('\\n', ' ').replace('\\xa0', ' ')  # Replace newlines and non-breaking spaces with regular spaces\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "        cleaned_text = re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', cleaned_text)  # Remove spaces between digits\n",
    "        cleaned_text = cleaned_text.strip()  # Remove leading and trailing spaces\n",
    "        return cleaned_text\n",
    "\n",
    "    html_soup = soup.body or soup  # Use the body of the HTML if it exists, otherwise use the whole soup\n",
    "    nested = ['ul', 'ol', 'dl', 'li', 'dt', 'dd']  # Tags that represent nested lists\n",
    "    for tag in html_soup.find_all(recursive=False):  # Iterate over top-level tags in the HTML\n",
    "        if tag.name == 'p':\n",
    "            chunks.append(clean_text(tag.get_text(separator=' ')))  # Clean and add paragraph text to chunks\n",
    "        elif tag.name == 'link':\n",
    "            continue  # Skip link tags\n",
    "        elif tag.name in nested:\n",
    "            list_items = tag.find_all('li')  # Find all list items\n",
    "            for li in list_items:\n",
    "                chunks.append(clean_text(li.get_text(separator=' ')))  # Clean and add each list item text to chunks\n",
    "        else:\n",
    "            chunks.append(str(tag))  # Add other tags as strings\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import uuid\n",
    "from haystack import Document\n",
    "from haystack import component\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, id: str, next: str = None):\n",
    "        self.id = id\n",
    "        \n",
    "class Section:\n",
    "    def __init__(self, name: str, type: str):\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.chunks = []\n",
    "        self.sections = []\n",
    "        \n",
    "class Page:\n",
    "    def __init__(self, title: str):\n",
    "        self.title = title\n",
    "        self.sections = []\n",
    "        self.chunks = []\n",
    "        \n",
    "@component\n",
    "class WikiPageChunker:\n",
    "    \"\"\"\n",
    "    A component that splits the content of Wikipedia pages into chunks.\n",
    "    - The document content is expected to be in HTML format fetched via wikipediaapi and\n",
    "    which has been run through TextFileToDocument converter.\n",
    "    - Each chunk is a paragraph, list, or table in the Wikipedia page.\n",
    "    - Each chunk is stored as a separate document with text in 'content' field\n",
    "    - Each chunk also stores title, h2, h3 etc in meta field.\n",
    "    - Custom component also creates a hierarchical structure of the chunks based on title, h2, h3 etc.\n",
    "    \"\"\"\n",
    "    @component.output_types(documents=List[Document], hierarchy=dict)\n",
    "    def run(self, documents: List[Document]):\n",
    "        chunks = []\n",
    "        hierarchy = {}\n",
    "        \n",
    "        for doc in documents:\n",
    "            page_title = doc.meta[\"file_path\"].replace(\".html\", \"\")\n",
    "            page = Page(page_title)\n",
    "            \n",
    "            html_content = doc.content\n",
    "            page_chunks = WikiPageChunks(html_content)\n",
    "            i = 0\n",
    "            current_h2 = \"\"\n",
    "            current_h3 = \"\"\n",
    "            current_h4 = \"\"\n",
    "            current_section = None\n",
    "            current_sub_section = None\n",
    "            current_sub_sub_section = None\n",
    "\n",
    "            for chunk in page_chunks:\n",
    "                if chunk == \"\":\n",
    "                    continue\n",
    "                if chunk.startswith(\"<h2>\"):\n",
    "                    current_h2 = chunk[4:-5]  # Extract text between <h2> and </h2>\n",
    "                    current_h3 = \"\"  # Reset h3 when a new h2 is found\n",
    "                    current_h4 = \"\"  # Reset h4 when a new h2 is found\n",
    "                    current_section = Section(current_h2, \"h2\")\n",
    "                    page.sections.append(current_section)\n",
    "                    current_sub_section = None\n",
    "                    current_sub_sub_section = None\n",
    "                elif chunk.startswith(\"<h3>\"):\n",
    "                    current_h3 = chunk[4:-5]  # Extract text between <h3> and </h3>\n",
    "                    current_h4 = \"\"  # Reset h4 when a new h3 is found\n",
    "                    if current_section:\n",
    "                        current_sub_section = Section(current_h3, \"h3\")\n",
    "                        current_section.sections.append(current_sub_section)\n",
    "                        current_sub_sub_section = None\n",
    "                elif chunk.startswith(\"<h4>\"):\n",
    "                    current_h4 = chunk[4:-5]  # Extract text between <h4> and </h4>\n",
    "                    if current_sub_section:\n",
    "                        current_sub_sub_section = Section(current_h4, \"h4\")\n",
    "                        current_sub_section.sections.append(current_sub_sub_section)\n",
    "                else:\n",
    "                    meta = {\n",
    "                        \"file_path\": doc.meta[\"file_path\"],\n",
    "                        \"source_id\": doc.id,\n",
    "                        \"split_id\": i,\n",
    "                        \"title\": \"Dinosaurs\"\n",
    "                    }\n",
    "                    if current_h2:\n",
    "                        meta[\"h2\"] = current_h2\n",
    "                    if current_h3:\n",
    "                        meta[\"h3\"] = current_h3\n",
    "                    if current_h4:\n",
    "                        meta[\"h4\"] = current_h4\n",
    "\n",
    "                    chunk_obj = Chunk(str(uuid.uuid4()))\n",
    "                    chunks.append(\n",
    "                        Document(\n",
    "                            id=chunk_obj.id,\n",
    "                            content=chunk,\n",
    "                            meta=meta\n",
    "                        )\n",
    "                    )\n",
    "                    if current_sub_sub_section:\n",
    "                        current_sub_sub_section.chunks.append(chunk_obj)\n",
    "                    elif current_sub_section:\n",
    "                        current_sub_section.chunks.append(chunk_obj)\n",
    "                    elif current_section:\n",
    "                        current_section.chunks.append(chunk_obj)\n",
    "                    else:\n",
    "                        page.chunks.append(chunk_obj)\n",
    "                    i += 1\n",
    "            \n",
    "            hierarchy[page_title] = self.page_to_dict(page)\n",
    "        \n",
    "        return {\"documents\": chunks, \"hierarchy\": hierarchy}\n",
    "\n",
    "    def page_to_dict(self, page: Page) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"title\": page.title,\n",
    "            \"sections\": [self.section_to_dict(section) for section in page.sections],\n",
    "            \"chunks\": [self.chunk_to_dict(chunk) for chunk in page.chunks],\n",
    "        }\n",
    "\n",
    "    def section_to_dict(self, section: Section) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"name\": section.name,\n",
    "            \"type\": section.type,\n",
    "            \"chunks\": [self.chunk_to_dict(chunk) for chunk in section.chunks],\n",
    "            \"sections\": [self.section_to_dict(sub_section) for sub_section in section.sections],\n",
    "        }\n",
    "\n",
    "    def chunk_to_dict(self, chunk: Chunk) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"id\": chunk.id,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jGraphCreator:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_graph(self, page_dict):\n",
    "        with self.driver.session() as session:\n",
    "            # Create Page node with UUID\n",
    "            page_uuid = str(uuid.uuid4())\n",
    "            page_query = \"\"\"\n",
    "            MERGE (p:Page {title: $title})\n",
    "            ON CREATE SET p.uuid = $uuid\n",
    "            RETURN p\n",
    "            \"\"\"\n",
    "            session.run(page_query, title=page_dict['title'], uuid=page_uuid)\n",
    "\n",
    "            # Create sections and chunks\n",
    "            self.create_sections_and_chunks(session, page_uuid, page_dict['sections'])\n",
    "            self.create_chunks(session, page_uuid, page_dict['chunks'])\n",
    "\n",
    "    def create_sections_and_chunks(self, session, parent_uuid, sections):\n",
    "        for section in sections:\n",
    "            section_uuid = str(uuid.uuid4())\n",
    "            section_labels = f\":Section:{section['type']}\"\n",
    "            \n",
    "            # Query to find either Page or Section as parent\n",
    "            section_query = f\"\"\"\n",
    "            MATCH (parent {{uuid: $parent_uuid}})\n",
    "            MERGE (s{section_labels} {{name: $name}})\n",
    "            ON CREATE SET s.uuid = $uuid\n",
    "            MERGE (parent)-[:HAS_SECTION]->(s)\n",
    "            RETURN s\n",
    "            \"\"\"\n",
    "            session.run(section_query, parent_uuid=parent_uuid, name=section['name'], uuid=section_uuid)\n",
    "\n",
    "            # Recursively create sub-sections and chunks\n",
    "            self.create_sections_and_chunks(session, section_uuid, section['sections'])\n",
    "            self.create_chunks(session, section_uuid, section['chunks'])\n",
    "\n",
    "    def create_chunks(self, session, parent_uuid, chunks):\n",
    "        first_chunk_created = False\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Use chunk['id'] as the UUID\n",
    "            chunk_uuid = chunk['id']\n",
    "            \n",
    "            chunk_query = \"\"\"\n",
    "            MATCH (parent {uuid: $parent_uuid})\n",
    "            MERGE (c:Chunk {uuid: $uuid})\n",
    "            MERGE (parent)-[:HAS_CHUNK]->(c)\n",
    "            RETURN c\n",
    "            \"\"\"\n",
    "            session.run(chunk_query, parent_uuid=parent_uuid, uuid=chunk_uuid)\n",
    "\n",
    "            # Create the FIRST_CHUNK relationship if first_chunk not yet created\n",
    "            if not first_chunk_created and i == 0:\n",
    "                first_chunk_query = \"\"\"\n",
    "                MATCH (parent {uuid: $parent_uuid}), (c:Chunk {uuid: $chunk_uuid})\n",
    "                MERGE (parent)-[:FIRST_CHUNK]->(c)\n",
    "                RETURN parent, c\n",
    "                \"\"\"\n",
    "                session.run(first_chunk_query, parent_uuid=parent_uuid, chunk_uuid=chunk['id'])\n",
    "                first_chunk_created = True\n",
    "\n",
    "        # Create NEXT relationships between chunks once all chunks are created\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Set the NEXT relationship\n",
    "            if i < len(chunks) - 1:\n",
    "                next_chunk_query = \"\"\"\n",
    "                MATCH (c1:Chunk {uuid: $uuid1}), (c2:Chunk {uuid: $uuid2})\n",
    "                MERGE (c1)-[:NEXT]->(c2)\n",
    "                RETURN c1, c2\n",
    "                \"\"\"\n",
    "                session.run(next_chunk_query, uuid1=chunk['id'], uuid2=chunks[i + 1]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class GraphCreatorComponent:\n",
    "    \"\"\"\n",
    "    A component that creates a graph in Neo4j from the hierarchical structure of Wikipedia page chunks.\n",
    "    \"\"\"\n",
    "    def run(self, hierarchy: dict):\n",
    "        neo4j_creator = Neo4jGraphCreator(\"bolt://localhost:7687\", \"neo4j\", \"neo4jpass\")\n",
    "        neo4j_creator.create_graph(hierarchy[\"Dinosaur\"])\n",
    "        neo4j_creator.close()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7281bf55d100>\n",
       "ðŸš… Components\n",
       "  - converter: TextFileToDocument\n",
       "  - splitter: WikiPageChunker\n",
       "  - embedder: OpenAIDocumentEmbedder\n",
       "  - w_writer: DocumentWriter\n",
       "  - e_writer: DocumentWriter\n",
       "  - graph_creator: GraphCreatorComponent\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - converter.documents -> splitter.documents (List[Document])\n",
       "  - splitter.documents -> embedder.documents (List[Document])\n",
       "  - splitter.documents -> e_writer.documents (List[Document])\n",
       "  - splitter.hierarchy -> graph_creator.hierarchy (dict)\n",
       "  - embedder.documents -> w_writer.documents (List[Document])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack_integrations.document_stores.weaviate.document_store import WeaviateDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack import Pipeline\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "\n",
    "converter = TextFileToDocument()\n",
    "splitter = WikiPageChunker()\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "w_store = WeaviateDocumentStore(url=\"http://localhost:8088\")\n",
    "w_writer = DocumentWriter(document_store=w_store)\n",
    "e_store = ElasticsearchDocumentStore(hosts= \"http://localhost:9200\")\n",
    "e_writer = DocumentWriter(document_store=e_store)\n",
    "graph_creator = GraphCreatorComponent()\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "indexing_pipeline.add_component(\"converter\", converter)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"w_writer\", w_writer)\n",
    "indexing_pipeline.add_component(\"e_writer\", e_writer)\n",
    "indexing_pipeline.add_component(\"graph_creator\", graph_creator)\n",
    "\n",
    "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder\", \"w_writer\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"e_writer.documents\")\n",
    "indexing_pipeline.connect(\"splitter.hierarchy\", \"graph_creator.hierarchy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_store.count_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.count_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/posthog/client.py:345: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().replace(tzinfo=tzutc())\n",
      "Calculating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:07<00:00,  1.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-3-small',\n",
       "   'usage': {'prompt_tokens': 20252, 'total_tokens': 20252}}},\n",
       " 'e_writer': {'documents_written': 258},\n",
       " 'w_writer': {'documents_written': 258}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing_pipeline.run(data={\"converter\": {\"sources\": [Path(\"Dinosaur.html\")]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=bceced15-011d-4c78-9be4-168e32244697, content: 'Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 201, 'title': 'Dinosaurs', 'h2': 'Paleobiology', 'h3': 'Size', 'h4': 'Largest and smallest'}, score: 0.0, embedding: vector of size 1536)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.filter_documents(filters = {\"field\": \"id\", \"operator\": \"==\", \"value\": \"bceced15-011d-4c78-9be4-168e32244697\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w_store cannot be filter queried without defining a schema it seems. e_store works fine when wanting to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=b47767a8-8844-4073-b16f-bf6be076525d, content: 'Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds,...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 231, 'title': 'Dinosaurs', 'h2': 'Origin of birds', 'h3': 'Soft anatomy'}, score: 0.0, embedding: vector of size 1536)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.filter_documents(filters = {\"field\": \"id\", \"operator\": \"==\", \"value\": \"b47767a8-8844-4073-b16f-bf6be076525d\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: Use Elasticsearch to fetch document by id. Weaviate schema hasn't been defined during document store init, therefore filtering does not work there.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartikeya/miniconda3/envs/argonk/lib/python3.12/site-packages/haystack/utils/filters.py:219: DeprecationWarning: The use of legacy (Haystack 1.x) filters is deprecated and will be removed in the future. Please use the new filter style as described in the documentation - https://docs.haystack.deepset.ai/docs/metadata-filtering\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id=75b8c578-e366-483d-8a1c-3e32a93dcc36, content: 'Dinosaurs diverged from their archosaur ancestors during the Middle to Late Triassic epochs, roughly...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 39, 'title': 'Dinosaurs', 'h2': 'Evolutionary history', 'h3': 'Origins and early evolution'}, score: 0.0, embedding: vector of size 1536)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.filter_documents(filters = {\"id\": \"75b8c578-e366-483d-8a1c-3e32a93dcc36\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above query is deprecated, use earlier filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a more complex query example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=bceced15-011d-4c78-9be4-168e32244697, content: 'Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 201, 'title': 'Dinosaurs', 'h2': 'Paleobiology', 'h3': 'Size', 'h4': 'Largest and smallest'}, score: 0.0, embedding: vector of size 1536),\n",
       " Document(id=7a844fcb-32a5-4dbf-bfd7-097d78f5a299, content: 'The tallest and heaviest dinosaur known from good skeletons is Giraffatitan brancai (previously clas...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 202, 'title': 'Dinosaurs', 'h2': 'Paleobiology', 'h3': 'Size', 'h4': 'Largest and smallest'}, score: 0.0, embedding: vector of size 1536),\n",
       " Document(id=5c19df63-6a0e-4d11-bfa5-825bbcbbc2fc, content: 'There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentar...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 203, 'title': 'Dinosaurs', 'h2': 'Paleobiology', 'h3': 'Size', 'h4': 'Largest and smallest'}, score: 0.0, embedding: vector of size 1536),\n",
       " Document(id=f15756e5-41b7-4bc7-b2cc-0a3aae739e62, content: 'The largest carnivorous dinosaur was Spinosaurus , reaching a length of 12.6 to 18 meters (41 to 59 ...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 204, 'title': 'Dinosaurs', 'h2': 'Paleobiology', 'h3': 'Size', 'h4': 'Largest and smallest'}, score: 0.0, embedding: vector of size 1536),\n",
       " Document(id=f39087f3-654f-42c8-b7b6-b9a671d028ad, content: 'The smallest dinosaur known is the bee hummingbird, with a length of only 5 centimeters (2.0 in) and...', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 205, 'title': 'Dinosaurs', 'h2': 'Paleobiology', 'h3': 'Size', 'h4': 'Largest and smallest'}, score: 0.0, embedding: vector of size 1536)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_store.filter_documents(filters={\n",
    "    \"operator\": \"AND\",\n",
    "    \"conditions\": [\n",
    "        {\"field\": \"meta.title\", \"operator\": \"==\", \"value\": \"Dinosaurs\"},\n",
    "        {\"field\": \"meta.h2\", \"operator\": \"==\", \"value\": \"Paleobiology\"},\n",
    "        {\"field\": \"meta.h3\", \"operator\": \"==\", \"value\": \"Size\"},\n",
    "        {\"field\": \"meta.h4\", \"operator\": \"==\", \"value\": \"Largest and smallest\"}\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note:\n",
    "- Most operations that can be done with Neo4j graph can be done using above filtering mechanism as well. Example - fetch all chunks of a given subsection deeply nested in a given title.\n",
    "- Exception being - find the previous and next chunk given a certain chunk id. Even this can be achieved with just Elasticsearch filter query, but would be much more natural with Neo4j.\n",
    "- In future, if more relations need to be added, for example - linking a chunk in one title with a chunk in another title, this can be done much more naturally in Neo4j.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack_integrations.components.retrievers.weaviate.embedding_retriever import WeaviateEmbeddingRetriever\n",
    "from haystack_integrations.components.retrievers.elasticsearch import ElasticsearchBM25Retriever\n",
    "\n",
    "\n",
    "question = \"What are heavy, quadrapedal thyreophorans?\"\n",
    "\n",
    "e_retriever = ElasticsearchBM25Retriever(document_store=e_store)\n",
    "w_retriever = WeaviateEmbeddingRetriever(document_store=w_store, top_k=1)\n",
    "\n",
    "document = Document(content=question)\n",
    "result = embedder.run(documents=[document])\n",
    "embedded_document = result[\"documents\"][0]\n",
    "embedding = embedded_document.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [Document(id=246e0812-17d0-49c6-8253-e69279ba95bf, content: 'â€ Eurypoda (heavy, quadrupedal thyreophorans)', meta: {'h3': 'Taxonomy', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 59.0, 'file_path': 'Dinosaur.html', 'title': 'Dinosaurs', 'h4': None, 'h2': 'Classification'}, score: 0.857631266117096, embedding: vector of size 1536)]}\n",
      "â€ Eurypoda (heavy, quadrupedal thyreophorans)\n"
     ]
    }
   ],
   "source": [
    "retrieval_result = w_retriever.run(query_embedding=embedding, top_k=1)\n",
    "print(retrieval_result)\n",
    "retrieved_documents = retrieval_result[\"documents\"]\n",
    "for doc in retrieved_documents:\n",
    "    print(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(id=246e0812-17d0-49c6-8253-e69279ba95bf, content: 'â€ Eurypoda (heavy, quadrupedal thyreophorans)', meta: {'file_path': 'Dinosaur.html', 'source_id': '93000a3fb02b99d2d115cd4042256d2f5db2a0ff3928927ca14465276534a75e', 'split_id': 59, 'title': 'Dinosaurs', 'h2': 'Classification', 'h3': 'Taxonomy'}, score: 19.630394, embedding: vector of size 1536)]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_fetched = e_retriever.run(query=question, top_k=1)\n",
    "\n",
    "elastic_fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: Neo4j also has this chunk under h3: Taxonomy. Check. Content cannot be compared as Neo4j is not storing text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (argonk)",
   "language": "python",
   "name": "argonk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
